{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "In this assignment you will ...\n",
    "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
    "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
    "\n",
    "IMPORTANT NOTE:\n",
    "\n",
    "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to import\n",
    "\n",
    "### Summary of each library imported and its function.\n",
    "\n",
    "1. **nltk.tokenize**: This function is used to tokenize text into individual words.\n",
    "2. **pos_tag**: Used for Parts of Speech tagging\n",
    "3. **stopwords**: provides a list of english words that mainly serve a grammatical function rather than provide meaning on their own.\n",
    "4. **WordNetLemmatizer**: performs word lemmatization which maps all the different forms of a word to its base form.\n",
    "5. **LabelEncoder**: Encodes the text into numerical variables that are digestible by the machine learning algorithms.\n",
    "6. **defaultdict**: useful in counting and aggregating data by using methods such as place holders for null key values.\n",
    "7. **wordnet**: used for clarifying meaning in words and finding appropriate synonyms.\n",
    "8. **TfidfVectorizer**: used for feature engineering, particularly on converting words into a numerical score using Term Frequency-Inverse Document Frequency (TF-IDF), to calculate the score of a word based on its frequency and rarity in appearance. The more frequent it is, the higher the score. The more rare it is, the higher the score. Words with high TF and IDF are ranked highest in a document.\n",
    "9. **model_selection**, naive_bayes, svm: importaing the machine learning algorithms to be used in training and testing the data. Naive Bayes is a probability based algorithm founded on Bayes principles that assumes each word is an independent event and assigns a probability to each word of its likelhood of belonging to a certain class. Support Vector Machines (SVM) find the most ideal between text categories that maximizes the distance from the boundary to enhance accuracy.\n",
    "10. **accuracy_score**: Determines the accuracy of the applied machine learning algorithms \n",
    "\n",
    "**NB If you are using Jupyter Notebook, ensure you install the libraries in the Anaconda environment, or by using the !pip function. See the code below for reference.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\zionn\\anaconda3\\envs\\ca02_bsan6070\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zionn\\anaconda3\\envs\\ca02_bsan6070\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\zionn\\anaconda3\\envs\\ca02_bsan6070\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zionn\\anaconda3\\envs\\ca02_bsan6070\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zionn\\anaconda3\\envs\\ca02_bsan6070\\lib\\site-packages (from requests) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Import all other necessary libraries. Your code below ...\n",
    "\n",
    "# Below are the additional libraries required for NLP text classification. \n",
    "# Additionally, requests, io, and nltk need to be imported.\n",
    "# Finally, we download the various nltk methods. Refer to code below.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "!pip install requests\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "nltk.download('punkt') # necessary on step (c), tokenization\n",
    "nltk.download('wordnet') # necessary on step (d), stop words\n",
    "nltk.download('omw-1.4') # necessary on step (d), stop words\n",
    "nltk.download('averaged_perceptron_tagger') # necessary on step (d), stop words\n",
    "nltk.download('stopwords') # necessary on step (d), stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to comments in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# In summary, a function is being created to load the numerous text files then extract the content from each one\n",
    "# before cleaning the text using methods such as tokenization. Next, the words are put into a Dictionary, which\n",
    "# counts the frequency of each word. Finally, the most common 3000 words are kept in the dictionary after a process \n",
    "# of elimination.\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "  all_words = []\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] #Collect txt files in root directory and store them in emails. \n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words #The FOR loop runs through each line, tokenizes the words, and stores them into a list called all_words.\n",
    "  dictionary = Counter(all_words) # Counter transforms \"all_words\" into a dictionary with the words and their frequency of appearance.\n",
    "  list_to_remove = list(dictionary) # A list is created to be used for removal of unecessary words or characters.\n",
    "\n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False: # FOR loop checks whether words in \"list_to_remove\" are alpha characters.\n",
    "      del dictionary[item] # If not an alpha character, it is deleted. \n",
    "    elif len(item) == 1:  # Identifies words that are one character long. \n",
    "      del dictionary[item] # If the word is one character long, it is removed. An example is 'a'. \n",
    "  dictionary = dictionary.most_common(3000) # A pre-defined list in the packages is used to retain the 3000 most common words.\n",
    "  return dictionary\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of function `def extract features`\n",
    "\n",
    "1. The first line within the function extracts the text files in the folders and stores them in a variable called '_Files_'.\n",
    "2. Next, it creates a variable called '_feature_matrix_', which is a matrix with the number of zeros equal to the length of text files loaded in the previous step in '_files_'.\n",
    "3. Another variable is created called '_train_labels_' that has the same number of zeros as the number of text files.\n",
    "4. A FOR loop is created that runs through each file, and another FOR loop within it goes through the 3rd line of each file.\n",
    "5. The 3rd line is then split into words, which are then cross referenced in the '_dictionary_' and if there is a match the word is assigned to '_wordID_'. The '_wordID_' variable will be used in feature engineering. \n",
    "6. A count of the words is done in the line of code '_features_matrix[docID,wordID] = words.count(word)_' to determine the frequency of the words appearing the both the email and dictionary.\n",
    "7. The lines of code from '_train_labels_' to '_lastToken_' extract the file names and store them in lastToken, which will be used to determine if the file is spam or not. \n",
    "8. AN IF statement is then used to identify any text starting with '_spmsg_' and labels it as 1. The docID keeps track of the email being processed. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train and Test Data\n",
    "\n",
    "The lines of code below are used for creating storage paths for the folders containing the training and testing emails, which will be used in the Naives Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter the \"path\" of your \"train_mails\" and \"test-mails\" FOLDERS in this cell ...\n",
    "# for example: TRAIN_DIR = '../../train-mails'\n",
    "#              TEST_DIR = '../../test-mails'\n",
    "\n",
    "TRAIN_DIR = 'C:/Users/zionn/msba/CA02/Data/train-mails'\n",
    "TEST_DIR = 'C:/Users/zionn/msba/CA02/Data/test-mails'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Features Matrix\n",
    "\n",
    "Next the most common 3000 words are called from the '_TRAIN_DIR_'  into dictionary. A statement indicating that the '_reading and processing emails from TRAIN and TEST folders_' is printed in case the algorithm takes a long time to run and anyone reading the code knows at what stage the process is.\n",
    "\n",
    "Apply the extract_features function is applied to the train and test directories and returns a feature matrix adn labels. The feature matrix shows the frequency of the 3000 most common words in each email and the labels show the type of emails, i.e. 1 for spam emails, and 0 for non-spam emails. \n",
    "\n",
    "Finally, the variables '_test_features_matrix_' and '_test_labels_' are used later to evaluate the performance of the model in classifying the data correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Gaussian Naive Bayes algorithm .....\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
    "# Your code below ...\n",
    "#\n",
    "#\n",
    "#\n",
    "# Your output should look like below if your code is right\n",
    "\n",
    "# Importing the Naives Algorithm and \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize the classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the classifier on the extracted features and corresponding labels\n",
    "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
    "gnb.fit(features_matrix, labels)\n",
    "print (\"Training completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n"
     ]
    }
   ],
   "source": [
    "# Use the trained classifier to predict the labels of the test data\n",
    "print (\"testing trained model to predict Test Data labels\")\n",
    "predicted_labels = gnb.predict(test_features_matrix)\n",
    "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation the model\n",
    "\n",
    "\n",
    "After the model has been successfully trained in the above code, we evaluate its performance in the code below. This is done by importing '_accuracy_score_' from the sklearn.metrics library. This function will compare the predicted values with the actual values and determine an accuracy score.  \n",
    "\n",
    "#### Results:\n",
    "\n",
    "The accuracy returned is 1.0, which implies a 100% accuracy. This figure needs to be confirmed because it is rare to get model score of this level of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import functions to evaluate performance of the model. \n",
    "\n",
    "# Accuracy score measures the accuracy of correct predictions. \n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
